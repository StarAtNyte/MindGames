textarena
requests==2.31.0
numpy==1.26.4
torch==2.4.0          # PyTorch for GPU inference
transformers==4.51.0  # Hugging Face transformers (Qwen3-8B support)
accelerate==0.30.1    # Fast model loading and memory optimization
bitsandbytes==0.43.1  # 4-bit/8-bit quantization support
sentencepiece==0.2.0  # Tokenization for Qwen models
protobuf==4.25.3      # Protocol buffers for model serialization
safetensors==0.4.3    # Safe model weight storage

# ============================================================================
# Installation instructions:
# ============================================================================
#
# 1. Install dependencies:
#    pip install -r requirements-local.txt
#
# 2. If you need a specific CUDA version:
#    For CUDA 11.8:
#      pip install torch --index-url https://download.pytorch.org/whl/cu118
#
#    For CUDA 12.1:
#      pip install torch --index-url https://download.pytorch.org/whl/cu121
#
# 3. Verify installation:
#    python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
#
# ============================================================================
# Hardware requirements:
# ============================================================================
#
# Minimum (4-bit quantization):
#   - RTX 4090 (24GB VRAM)
#   - ~4-5GB VRAM usage
#   - Inference: ~2-5 seconds per turn
#
# Recommended (8-bit quantization):
#   - A100 40GB/80GB
#   - ~8-10GB VRAM usage
#   - Inference: ~1-3 seconds per turn
#
# Optimal (full precision):
#   - A100 80GB or H100
#   - ~16GB VRAM usage
#   - Inference: ~0.5-2 seconds per turn
#
# ============================================================================
